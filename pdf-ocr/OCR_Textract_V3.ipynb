{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c712e91f",
   "metadata": {},
   "source": [
    "### Textract Optical Character Recognition for PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ac6ebf",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f8defad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS imports\n",
    "import boto3\n",
    "import time\n",
    "\n",
    "# Processing imports\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import shutil\n",
    "\n",
    "# Image imports\n",
    "import PIL.Image as Image\n",
    "from pdf2image import convert_from_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d0e2e1",
   "metadata": {},
   "source": [
    "#### Upload Documents to the AWS Bucket\n",
    "\n",
    "The below may take a few minutes to run, all documents in the specified folder will be uploaded to the cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1bcdbd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to the directory containing the pdfs\n",
    "input_dir = \"/Users/nicolebrye/Desktop/HGC/Data_Management/pdf-ocr/SD4000s_20220426_24-2\"\n",
    "\n",
    "# Set to the directory where the pdfs will be stored\n",
    "output_dir = \"/Users/nicolebrye/Desktop/HGC/Data_Management/pdf-ocr/Test_PDFs\"\n",
    "\n",
    "# Set to the name of the AWS S3 bucket containing the pdfs\n",
    "s3BucketName = \"pdf-ocr-bucket\"\n",
    "\n",
    "# Set to the name of the AWS S3 bucket containing sub-images if needed\n",
    "s3ImageName  = \"pdf-ocr-images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3841f299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the documents to the cloud\n",
    "PDFS = os.listdir(input_dir)\n",
    "\n",
    "# Call s3 as a resource\n",
    "s3 = boto3.resource(\"s3\")\n",
    "\n",
    "for pdf in PDFS:\n",
    "    data = open(os.path.join(input_dir, pdf), \"rb\")\n",
    "    s3.Bucket(s3BucketName).put_object(Key = pdf, Body = data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26aaea5c",
   "metadata": {},
   "source": [
    "#### Important Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9337c1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names for output\n",
    "cols = ['Batch_UID', 'Exam_UID', 'InstrumentModel', 'InstrumentSerialNumber', \n",
    "        'InstrumentSoftwareVersion', 'PatientID', 'GIVEN_NAME', 'LAST_NAME', \n",
    "        'StudyCode', 'aeDOB', 'Eye', 'SeriesDateTime',\n",
    "        'aeExamDate', 'aeExamTime', 'ExamDuration', 'aeIsShileyClinicHFAExam', 'aeDIGSTestType',\n",
    "        'TestType', 'TestPattern', 'TestStrategy', 'StimulusColor', 'StimulusSize', \n",
    "        'BackgroundColor', 'FixationTarget', 'FixationMonitor', 'TrialRXSphereRaw', \n",
    "        'TrialRXCylRaw', 'TrialRXAxisRaw', 'PupilDiameter', 'VAType', 'BlindSpotX','BlindSpotY',\n",
    "        'BlindSpotStimulusSize', 'FalseNegativePercent', 'FalsePositivePercent',\n",
    "        'aeFixationCheckPercentage', 'FovealResult', 'FovealThreshold', 'ClinicalNotes',\n",
    "        'SFStatus', 'SF', 'SFProb', 'SWAPFTGeneralHeight', 'GHTType', 'MD', 'MDProb', 'PSD',\n",
    "        'PSDProb', 'CPSD', 'CPSDProb', 'FovealThresholdProb', 'aePDPCen4LT5Count', \n",
    "        'aeHasHighRawThreshold', 'FLAGAssessment', 'FLAGSeverity', 'AGISScore', 'AGISNas', \n",
    "        'AGISInf', 'AGISSup', 'GHSupThrSum', 'GHSupThrMean', 'GHSupThrStd', 'GHSupTDSum',\n",
    "        'GHSupTDMean', 'GHSupTDStd', 'GHSupPDSum', 'GHSupPDMean', 'GHSupPDStd', 'GHSupPDCntLT50p',\n",
    "        'GHSupPDCntLT10p', 'GHSupNasThrSum', 'GHSupNasThrMean', 'GHSupNasThrStd', 'GHSupNasTDSum',\n",
    "        'GHSupNasTDMean', 'GHSupNasTDStd', 'GHSupNasPDSum', 'GHSupNasPDMean', 'GHSupNasPDStd',\n",
    "        'GHSupNasPDCntLT50p', 'GHSupNasPDCntLT10p', 'GHInfThrSum', 'GHInfThrMean', 'GHInfThrStd',\n",
    "        'GHInfTDSum', 'GHInfTDMean', 'GHInfTDStd', 'GHInfPDSum', 'GHInfPDMean', 'GHInfPDStd',\n",
    "        'GHInfPDCntLT50p', 'GHInfPDCntLT10p', 'GHInfNasThrSum', 'GHInfNasThrMean', 'GHInfNasThrStd',\n",
    "        'GHInfNasTDSum', 'GHInfNasTDMean', 'GHInfNasTDStd', 'GHInfNasPDSum', 'GHInfNasPDMean',\n",
    "        'GHInfNasPDStd', 'GHInfNasPDCntLT50p', 'GHInfNasPDCntLT10p', 'GHCentralThrSum',\n",
    "        'GHCentralThrMean', 'GHCentralThrStd', 'GHCentralTDSum', 'GHCentralTDMean', 'GHCentralTDStd',\n",
    "        'GHCentralPDSum', 'GHCentralPDMean', 'GHCentralPDStd', 'GHCentralPDCntLT50p', \n",
    "        'GHCentralPDCntLT10p', 'GHTemporalThrSum', 'GHTemporalThrMean', 'GHTemporalThrStd',\n",
    "        'GHTemporalTDSum', 'GHTemporalTDMean', 'GHTemporalTDStd', 'GHTemporalPDSum', \n",
    "        'GHTemporalPDMean', 'GHTemporalPDStd', 'GHTemporalPDCntLT50p', 'GHTemporalPDCntLT10p',\n",
    "        'N9_S27_Thr', 'N3_S27_Thr', 'T3_S27_Thr', 'T9_S27_Thr', 'N15_S21_Thr', 'N9_S21_Thr',\n",
    "        'N3_S21_Thr', 'T3_S21_Thr', 'T9_S21_Thr', 'T15_S21_Thr', 'N21_S15_Thr', 'N15_S15_Thr', \n",
    "        'N9_S15_Thr', 'N3_S15_Thr', 'T3_S15_Thr', 'T9_S15_Thr', 'T15_S15_Thr', 'T21_S15_Thr',\n",
    "        'N27_S9_Thr', 'N21_S9_Thr', 'N15_S9_Thr', 'N9_S9_Thr', 'N3_S9_Thr', 'T3_S9_Thr', 'T9_S9_Thr',\n",
    "        'T15_S9_Thr', 'T21_S9_Thr', 'T27_S9_Thr', 'N27_S3_Thr', 'N21_S3_Thr', 'N15_S3_Thr', \n",
    "        'N9_S3_Thr',  'N3_S3_Thr', 'T3_S3_Thr', 'T9_S3_Thr', 'T21_S3_Thr', 'T27_S3_Thr', \n",
    "        'N27_I3_Thr', 'N21_I3_Thr', 'N15_I3_Thr', 'N9_I3_Thr', 'N3_I3_Thr', 'T3_I3_Thr', 'T9_I3_Thr', \n",
    "        'T21_I3_Thr', 'T27_I3_Thr', 'N27_I9_Thr', 'N21_I9_Thr', 'N15_I9_Thr', 'N9_I9_Thr', \n",
    "        'N3_I9_Thr', 'T3_I9_Thr', 'T9_I9_Thr', 'T15_I9_Thr', 'T21_I9_Thr', 'T27_I9_Thr', \n",
    "        'N21_I15_Thr', 'N15_I15_Thr', 'N9_I15_Thr', 'N3_I15_Thr', 'T3_I15_Thr', 'T9_I15_Thr', \n",
    "        'T15_I15_Thr', 'T21_I15_Thr', 'N15_I21_Thr', 'N9_I21_Thr', 'N3_I21_Thr', 'T3_I21_Thr',\n",
    "        'T9_I21_Thr', 'T15_I21_Thr', 'N9_I27_Thr', 'N3_I27_Thr', 'T3_I27_Thr', 'T9_I27_Thr', \n",
    "        'N9_S27_TD', 'N3_S27_TD', 'T3_S27_TD', 'T9_S27_TD', 'N15_S21_TD', 'N9_S21_TD', 'N3_S21_TD', \n",
    "        'T3_S21_TD', 'T9_S21_TD', 'T15_S21_TD', 'N21_S15_TD', 'N15_S15_TD', 'N9_S15_TD', 'N3_S15_TD',\n",
    "        'T3_S15_TD', 'T9_S15_TD', 'T15_S15_TD', 'T21_S15_TD', 'N27_S9_TD', 'N21_S9_TD', 'N15_S9_TD', \n",
    "        'N9_S9_TD', 'N3_S9_TD', 'T3_S9_TD', 'T9_S9_TD', 'T15_S9_TD', 'T21_S9_TD', 'T27_S9_TD', \n",
    "        'N27_S3_TD', 'N21_S3_TD', 'N15_S3_TD', 'N9_S3_TD', 'N3_S3_TD', 'T3_S3_TD', 'T9_S3_TD', \n",
    "        'T21_S3_TD', 'T27_S3_TD', 'N27_I3_TD', 'N21_I3_TD', 'N15_I3_TD', 'N9_I3_TD', 'N3_I3_TD',\n",
    "        'T3_I3_TD', 'T9_I3_TD', 'T21_I3_TD', 'T27_I3_TD', 'N27_I9_TD', 'N21_I9_TD', 'N15_I9_TD', \n",
    "        'N9_I9_TD', 'N3_I9_TD', 'T3_I9_TD', 'T9_I9_TD', 'T15_I9_TD', 'T21_I9_TD', 'T27_I9_TD',\n",
    "        'N21_I15_TD', 'N15_I15_TD', 'N9_I15_TD', 'N3_I15_TD', 'T3_I15_TD', 'T9_I15_TD', 'T15_I15_TD',\n",
    "        'T21_I15_TD', 'N15_I21_TD', 'N9_I21_TD', 'N3_I21_TD', 'T3_I21_TD', 'T9_I21_TD', 'T15_I21_TD',\n",
    "        'N9_I27_TD', 'N3_I27_TD', 'T3_I27_TD', 'T9_I27_TD', 'N9_S27_PD', 'N3_S27_PD', 'T3_S27_PD', \n",
    "        'T9_S27_PD', 'N15_S21_PD', 'N9_S21_PD', 'N3_S21_PD', 'T3_S21_PD', 'T9_S21_PD', 'T15_S21_PD',\n",
    "        'N21_S15_PD', 'N15_S15_PD', 'N9_S15_PD', 'N3_S15_PD', 'T3_S15_PD', 'T9_S15_PD', 'T15_S15_PD',\n",
    "        'T21_S15_PD', 'N27_S9_PD', 'N21_S9_PD', 'N15_S9_PD', 'N9_S9_PD', 'N3_S9_PD', 'T3_S9_PD', \n",
    "        'T9_S9_PD', 'T15_S9_PD', 'T21_S9_PD', 'T27_S9_PD', 'N27_S3_PD', 'N21_S3_PD', 'N15_S3_PD', \n",
    "        'N9_S3_PD', 'N3_S3_PD', 'T3_S3_PD', 'T9_S3_PD', 'T21_S3_PD', 'T27_S3_PD', 'N27_I3_PD',\n",
    "        'N21_I3_PD', 'N15_I3_PD', 'N9_I3_PD', 'N3_I3_PD', 'T3_I3_PD', 'T9_I3_PD', 'T21_I3_PD', \n",
    "        'T27_I3_PD', 'N27_I9_PD', 'N21_I9_PD', 'N15_I9_PD', 'N9_I9_PD', 'N3_I9_PD', 'T3_I9_PD', \n",
    "        'T9_I9_PD', 'T15_I9_PD', 'T21_I9_PD', 'T27_I9_PD', 'N21_I15_PD', 'N15_I15_PD', 'N9_I15_PD',\n",
    "        'N3_I15_PD', 'T3_I15_PD', 'T9_I15_PD', 'T15_I15_PD', 'T21_I15_PD', 'N15_I21_PD', 'N9_I21_PD', \n",
    "        'N3_I21_PD', 'T3_I21_PD', 'T9_I21_PD', 'T15_I21_PD', 'N9_I27_PD', 'N3_I27_PD', 'T3_I27_PD',\n",
    "        'T9_I27_PD', 'N9_S27_TDP', 'N3_S27_TDP', 'T3_S27_TDP', 'T9_S27_TDP', 'N15_S21_TDP', 'N9_S21_TDP', \n",
    "        'N3_S21_TDP', 'T3_S21_TDP', 'T9_S21_TDP', 'T15_S21_TDP', 'N21_S15_TDP', 'N15_S15_TDP', \n",
    "        'N9_S15_TDP', 'N3_S15_TDP', 'T3_S15_TDP', 'T9_S15_TDP', 'T15_S15_TDP', 'T21_S15_TDP', \n",
    "        'N27_S9_TDP', 'N21_S9_TDP', 'N15_S9_TDP', 'N9_S9_TDP', 'N3_S9_TDP', 'T3_S9_TDP', 'T9_S9_TDP', \n",
    "        'T15_S9_TDP', 'T21_S9_TDP', 'T27_S9_TDP', 'N27_S3_TDP', 'N21_S3_TDP', 'N15_S3_TDP', 'N9_S3_TDP', \n",
    "        'N3_S3_TDP', 'T3_S3_TDP', 'T9_S3_TDP', 'T21_S3_TDP', 'T27_S3_TDP', 'N27_I3_TDP', 'N21_I3_TDP',\n",
    "        'N15_I3_TDP', 'N9_I3_TDP', 'N3_I3_TDP', 'T3_I3_TDP', 'T9_I3_TDP', 'T21_I3_TDP', 'T27_I3_TDP',\n",
    "        'N27_I9_TDP', 'N21_I9_TDP', 'N15_I9_TDP', 'N9_I9_TDP', 'N3_I9_TDP', 'T3_I9_TDP', 'T9_I9_TDP',\n",
    "        'T15_I9_TDP', 'T21_I9_TDP', 'T27_I9_TDP', 'N21_I15_TDP', 'N15_I15_TDP', 'N9_I15_TDP', \n",
    "        'N3_I15_TDP', 'T3_I15_TDP', 'T9_I15_TDP', 'T15_I15_TDP', 'T21_I15_TDP', 'N15_I21_TDP', \n",
    "        'N9_I21_TDP', 'N3_I21_TDP', 'T3_I21_TDP', 'T9_I21_TDP', 'T15_I21_TDP', 'N9_I27_TDP', \n",
    "        'N3_I27_TDP', 'T3_I27_TDP', 'T9_I27_TDP', 'N9_S27_PDP', 'N3_S27_PDP', 'T3_S27_PDP', \n",
    "        'T9_S27_PDP', 'N15_S21_PDP', 'N9_S21_PDP', 'N3_S21_PDP', 'T3_S21_PDP', 'T9_S21_PDP', \n",
    "        'T15_S21_PDP', 'N21_S15_PDP', 'N15_S15_PDP', 'N9_S15_PDP', 'N3_S15_PDP', 'T3_S15_PDP', \n",
    "        'T9_S15_PDP', 'T15_S15_PDP', 'T21_S15_PDP', 'N27_S9_PDP', 'N21_S9_PDP', 'N15_S9_PDP',\n",
    "        'N9_S9_PDP', 'N3_S9_PDP', 'T3_S9_PDP', 'T9_S9_PDP', 'T15_S9_PDP', 'T21_S9_PDP', 'T27_S9_PDP', \n",
    "        'N27_S3_PDP', 'N21_S3_PDP', 'N15_S3_PDP', 'N9_S3_PDP', 'N3_S3_PDP', 'T3_S3_PDP', \n",
    "        'T9_S3_PDP', 'T21_S3_PDP', 'T27_S3_PDP', 'N27_I3_PDP', 'N21_I3_PDP', 'N15_I3_PDP',\n",
    "        'N9_I3_PDP', 'N3_I3_PDP', 'T3_I3_PDP', 'T9_I3_PDP', 'T21_I3_PDP', 'T27_I3_PDP',\n",
    "        'N27_I9_PDP', 'N21_I9_PDP', 'N15_I9_PDP', 'N9_I9_PDP', 'N3_I9_PDP', 'T3_I9_PDP', \n",
    "        'T9_I9_PDP', 'T15_I9_PDP', 'T21_I9_PDP', 'T27_I9_PDP', 'N21_I15_PDP', 'N15_I15_PDP',\n",
    "        'N9_I15_PDP', 'N3_I15_PDP', 'T3_I15_PDP', 'T9_I15_PDP', 'T15_I15_PDP', 'T21_I15_PDP',\n",
    "        'N15_I21_PDP', 'N9_I21_PDP', 'N3_I21_PDP', 'T3_I21_PDP', 'T9_I21_PDP', 'T15_I21_PDP',\n",
    "        'N9_I27_PDP', 'N3_I27_PDP', 'T3_I27_PDP', 'T9_I27_PDP', 'cAutoQCStatus', 'QCFieldUsable',\n",
    "        'QCReliable', 'cQCFN33Status', 'cQCAHSManualStatus', 'cQCRimArtifactStatus',\n",
    "        'cQCInattentionStatus', 'cQCLearningEffectStatus', 'cQCFatigueStatus', 'cQCFixationStatus', \n",
    "        'cQCOtherDefectStatus', 'cQCUnreliableByTechnicianStatus', 'cQCUnaccPupilSizeStatus','VFI',\n",
    "        'kPrevUsable_ExamTimeStamp', 'aeExamTimeStamp', 'kNextUsable_ExamTimeStamp', \n",
    "        'kPrevUsable_FLAGAssessment', 'kNextUsable_FLAGAssessment', \n",
    "        'cFLAG_Confirmation_Status_ByTestType', 'cIsABNORMAL_FLAG_Confirmed_ByTestType',\n",
    "        'cIsNORMAL_FLAG_Confirmed_ByTestType', 'cCnt_TDP_LessThan5', 'cCnt_PDP_LessThan5',\n",
    "        'cCnt_TDP_LessThan2', 'cCnt_PDP_LessThan2', 'cCnt_TDP_LessThan1', 'cCnt_PDP_LessThan1',\n",
    "        'cCnt_TDP_LessThan05', 'cCnt_PDP_LessThan05', 'kUsedADAGESBL09','sFLAGAbn3ConsecConfirmed',\n",
    "        'sFLAGAbn3ConsecUnconfirmed', 'sFLAGNorm3ConsecConfirmed', 'sFLAGNorm3ConsecUnconfirmed',\n",
    "        'LowPatientReliabilityStatus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "63c278c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the s3 bucket\n",
    "bucket = s3.Bucket(s3BucketName)\n",
    "\n",
    "# Loop through all the files in the bucket and append them to a list\n",
    "files = []\n",
    "for i in bucket.objects.all():\n",
    "    files.append(i.key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d203fd0",
   "metadata": {},
   "source": [
    "#### Extract Text Function\n",
    "\n",
    "This function takes a PDF or an image from an S3 bucket, and extracts its text using AWS Textract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "af9177e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(bucket_name, filename):\n",
    "    \n",
    "    response = client.start_document_text_detection(\n",
    "        DocumentLocation={\n",
    "            'S3Object': {\n",
    "                'Bucket': bucket_name,\n",
    "                'Name': filename\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    jobId = response[\"JobId\"]\n",
    "    \n",
    "    # Extract text\n",
    "    textractmodule = client.get_document_text_detection(JobId=jobId)\n",
    "    \n",
    "    # Check if the OCR is complete\n",
    "    status = textractmodule[\"JobStatus\"]\n",
    "    \n",
    "    while(status == \"IN_PROGRESS\"):\n",
    "        time.sleep(5)\n",
    "        textractmodule = client.get_document_text_detection(JobId=jobId)\n",
    "        status = textractmodule[\"JobStatus\"]\n",
    "        \n",
    "    # Extract text from the module\n",
    "    text = []\n",
    "    for block in textractmodule[\"Blocks\"]:\n",
    "        if block[\"BlockType\"] == \"LINE\":\n",
    "            text.append(block[\"Text\"])\n",
    "            \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a9a9ad",
   "metadata": {},
   "source": [
    "#### Key Value Function\n",
    "\n",
    "This function converts the raw text of the PDF to key value pairs which can have information extracted easily. This must be called before the raw text function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "78470352",
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_value_output(text):\n",
    "    \n",
    "    # Convert text into key value pairs\n",
    "    temp = {}\n",
    "    for i in range(len(text)):\n",
    "    \n",
    "        val = text[i].split(\":\")\n",
    "    \n",
    "        if len(val) == 2:\n",
    "            if val[1] == \"\":\n",
    "                temp[val[0].strip()] = text[i+1].strip().strip(\",\")\n",
    "            else:\n",
    "                temp[val[0].strip()] = val[1].strip().strip(\",\")\n",
    "                \n",
    "    # Save the keys in a Series\n",
    "    KEYS = pd.Series(temp.keys())\n",
    "                \n",
    "    # Initialize pandas DataFrame\n",
    "    output = pd.DataFrame(columns=cols, index=[0])\n",
    "    \n",
    "    # Important dates\n",
    "    DOB = pd.to_datetime(temp[\"Date of Birth\"])\n",
    "    VIS = pd.to_datetime(temp[\"Date\"])\n",
    "\n",
    "    # Patient Info\n",
    "    output[\"PatientID\"]  = temp[\"Patient ID\"]\n",
    "    \n",
    "    try:\n",
    "        output[\"GIVEN_NAME\"] = temp[\"Patient\"].split(\",\")[1]\n",
    "    except IndexError:\n",
    "        output[\"GIVEN_NAME\"] = \"\"\n",
    "        \n",
    "    output[\"LAST_NAME\"]  = temp[\"Patient\"]\n",
    "    output[\"aeDOB\"] = str(DOB.month) + \"/\" + str(DOB.day) + \"/\" + str(DOB.year) #[-2:]\n",
    "    \n",
    "    # Visit Info\n",
    "    output[\"aeExamTime\"] = temp[\"Time\"].split()[0]\n",
    "    output[\"aeExamDate\"] = str(VIS.month) + \"/\" + str(VIS.day) + \"/\" + str(VIS.year) #[-2:]\n",
    "    output[\"TestStrategy\"] = temp[\"Strategy\"]\n",
    "    output[\"ExamDuration\"] = temp[\"Test Duration\"]\n",
    "    \n",
    "    # Eye Info\n",
    "    try:\n",
    "        output[\"VAType\"] = float(temp[\"Visual Acuity\"])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    MDKEY  = KEYS[KEYS.str.contains(\"MD\").idxmax()]\n",
    "    PSDKEY = KEYS[KEYS.str.contains(\"PSD\").idxmax()]\n",
    "    \n",
    "    # MD and MDProb\n",
    "    if temp[MDKEY].endswith(\"dB\"):\n",
    "        output[\"MD\"]     = temp[MDKEY].strip(\" dB\")\n",
    "        output[\"MDProb\"] = \"Not Significant\"\n",
    "        \n",
    "    elif temp[MDKEY].endswith(\"%\"):\n",
    "        output[\"MD\"]     = temp[MDKEY].split(\" dB \")[0]\n",
    "        output[\"MDProb\"] = temp[MDKEY].split(\" dB \")[1]\n",
    "\n",
    "    \n",
    "    # PSD and PSDProb\n",
    "    if temp[PSDKEY].endswith(\"dB\"):\n",
    "        output[\"PSD\"]     = temp[PSDKEY].strip(\" dB\")\n",
    "        output[\"PSDProb\"] = \"Not Significant\"\n",
    "        \n",
    "    elif temp[PSDKEY].endswith(\"%\"):\n",
    "        output[\"PSD\"]     = temp[PSDKEY].split(\" dB \")[0]\n",
    "        output[\"PSDProb\"] = temp[PSDKEY].split(\" dB \")[1]\n",
    "\n",
    "    \n",
    "    # Other Info\n",
    "    output[\"FixationTarget\"]  = temp[\"Fixation Target\"]\n",
    "    output[\"FixationMonitor\"] = temp[\"Fixation Monitor\"]\n",
    "    output[\"StimulusSize\"]    = temp[\"Stimulus\"].split(\", \")[0]\n",
    "    output[\"StimulusColor\"]   = temp[\"Stimulus\"].split()[1]\n",
    "    output[\"BackgroundColor\"] = output[\"StimulusColor\"] + \" (\" + temp[\"Background\"] + \")\"\n",
    "    output[\"PupilDiameter\"]   = temp[\"Pupil Diameter\"].split()[0]\n",
    "    output[\"FovealThreshold\"] = temp[\"Fovea\"].split()[0]\n",
    "    output[\"GHTType\"]         = temp[\"GHT\"]\n",
    "    \n",
    "    try:\n",
    "        output[\"VFI\"] = temp[\"VFI\"].strip(\"%\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    err, trials = temp[\"Fixation Losses\"].split()[0].split(\"/\")\n",
    "    output[\"aeFixationCheckPercentage\"] = round(float(err) / float(trials) * 100, 2)\n",
    "    output[\"FalsePositivePercent\"] = temp[\"False POS Errors\"].strip(\" XX\").strip(\"%\")\n",
    "    output[\"FalseNegativePercent\"] = temp[\"False NEG Errors\"].strip(\" XX\").strip(\"%\")\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da1b753",
   "metadata": {},
   "source": [
    "#### Close-Up Function\n",
    "\n",
    "This function is used for the Total Deviation and Pattern Deviation points that the AWS OCR would otherwise not be able to extract. It takes close up screenshots of the pdf in order to more accurately extract the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fc0ceda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def close_up(bucket_name, filename, mode = 1):\n",
    "    \n",
    "    img_fn = filename.strip(\".pdf\") + \".jpg\"\n",
    "    img_fp = os.path.join(input_dir, \"images\")\n",
    "    \n",
    "    # Convert the pdf to an image\n",
    "    path = os.path.join(input_dir, filename)\n",
    "    img  = np.array(convert_from_path(path)[0])\n",
    "    \n",
    "    # Subset the image for TD and PD\n",
    "    if mode == 1:\n",
    "        img1 = img[1120:1390, 180:480]\n",
    "        img2 = img[1120:1390, 690:990]\n",
    "    if mode == 2:\n",
    "        img1 = img[1130:1390, 190:480]\n",
    "        img2 = img[1130:1390, 700:990]\n",
    "    \n",
    "    # Create a new directory to store the images if it doesn't exist\n",
    "    if not os.path.isdir(img_fp):\n",
    "        os.makedirs(img_fp)\n",
    "    \n",
    "    # Save the images to upload them to the s3 bucket\n",
    "    Image.fromarray(img1).save(os.path.join(img_fp, (\"TD_\" + img_fn)))\n",
    "    Image.fromarray(img2).save(os.path.join(img_fp, (\"PD_\" + img_fn)))\n",
    "    \n",
    "    # Upload images to the s3 bucket\n",
    "    data1 = open(os.path.join(img_fp, (\"TD_\" + img_fn)), \"rb\")\n",
    "    data2 = open(os.path.join(img_fp, (\"PD_\" + img_fn)), \"rb\")\n",
    "    \n",
    "    s3.Bucket(s3ImageName).put_object(Key = (\"TD_\" + img_fn), Body = data1)\n",
    "    s3.Bucket(s3ImageName).put_object(Key = (\"PD_\" + img_fn), Body = data2)\n",
    "    \n",
    "    # Extract text from the images\n",
    "    text1 = extract_text(s3ImageName, (\"TD_\" + img_fn))\n",
    "    text2 = extract_text(s3ImageName, (\"PD_\" + img_fn))\n",
    "    \n",
    "    # Define a helper function to clean the values\n",
    "    def _clean_points(values):\n",
    "        \n",
    "        output = []\n",
    "        for i in values:\n",
    "            clean = re.findall(\"(\\d+|\\-\\d+)\", i)\n",
    "            output = output + clean\n",
    "            \n",
    "        return output\n",
    "    \n",
    "    TD = _clean_points(text1)\n",
    "    PD = _clean_points(text2)\n",
    "    \n",
    "    return TD, PD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8a1f27",
   "metadata": {},
   "source": [
    "#### Raw Text Function\n",
    "\n",
    "This function takes an output DataFrame provided by the key value function and a list of text from the pdf. This function analyzes the raw text to extract values from the pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "596156b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_text_output(output, text, filename):\n",
    "    \n",
    "    # Find the thr points as well as other important information\n",
    "    POINTS = []\n",
    "    \n",
    "    for i, x in enumerate(text):\n",
    "    \n",
    "        # Use regexes to extract the necessary information\n",
    "        point  = re.search(\"^\\-?\\<?\\d?[\\d?\\s?\\-?]*\\d$\", x)\n",
    "        eye    = re.search(\"(^OD$|^OS$)\", x)\n",
    "        test   = re.search(\"\\s\\d{2}-\\d\\s\", x)\n",
    "        inst   = re.search(\"HFA\", x)\n",
    "        vers   = re.search(\"Version\", x)\n",
    "        rx     = re.search(\"Rx:\\s.*\", x)\n",
    "        field  = re.search(\"°\", x)\n",
    "        status = re.search(\"[A-z\\s]+\\*{3}\", x)\n",
    "\n",
    "        # Format the information extracted from regexes\n",
    "        if point:\n",
    "            clean = re.findall(\"(<0|\\d+|\\-\\d+)\", x)\n",
    "            POINTS = POINTS + clean\n",
    "\n",
    "        if eye:\n",
    "            Eye = eye.group(0)\n",
    "\n",
    "        if test:\n",
    "            words = x.split()\n",
    "\n",
    "            TestPattern = \" \".join([words[0], words[1]])\n",
    "            TestType    = words[2]\n",
    "\n",
    "        if inst:\n",
    "            inst_info = x.split()\n",
    "\n",
    "            InstrumentName = \" \".join([inst_info[0], inst_info[1]])\n",
    "            InstrumentModel = inst_info[-1].split(\"-\")[0]\n",
    "            InstrumentSerialNumber = inst_info[-1].split(\"-\")[1][:4]\n",
    "            InstrumentSoftwareVersion = inst_info[-1].split(\"/\")[-1]\n",
    "\n",
    "        if vers:\n",
    "            Version = x.split()[1]  \n",
    "\n",
    "        if rx:\n",
    "            rxval = rx.group(0)\n",
    "            TrialRXSphereRaw = re.findall(\"(\\d+\\.\\d+)\\s?DS\", rxval)[0]\n",
    "            \n",
    "            try:\n",
    "                TrialRXCylRaw    = re.findall(\"(\\d+\\.\\d+)\\s?DC\", rxval)[0]\n",
    "            except:\n",
    "                TrialRXCylRaw    = \"\"\n",
    "                \n",
    "            try:\n",
    "                TrialRXAxisRaw   = re.findall(\"X(.*)\", rxval)[0]\n",
    "            except:\n",
    "                TrialRXAxisRaw   = \"\"\n",
    "\n",
    "        if field:\n",
    "            FieldSize = x.strip()\n",
    "            \n",
    "        if status:\n",
    "            LowPatientReliabilityStatus = status.group(0).strip(\"***\").strip()\n",
    "            \n",
    "    # Order the plot points\n",
    "    if Eye == \"OS\":\n",
    "        ORDER = ['T9_S21', 'T3_S21', 'N3_S21', 'N9_S21', 'T15_S15',\n",
    "                 'T9_S15', 'T3_S15', 'N3_S15', 'N9_S15', 'N15_S15',\n",
    "                 'T21_S9', 'T15_S9', 'T9_S9', 'T3_S9', 'N3_S9','N9_S9',\n",
    "                 'N15_S9', 'N21_S9', 'T21_S3', 'T15_S3', 'T9_S3','T3_S3',\n",
    "                 'N3_S3', 'N9_S3', 'N15_S3', 'N21_S3', 'N27_S3', \n",
    "                 'T21_I3', 'T15_I3', 'T9_I3', 'T3_I3', 'N3_I3', 'N9_I3',\n",
    "                 'N15_I3', 'N21_I3', 'N27_I3', 'T21_I9', 'T15_I9',\n",
    "                 'T9_I9', 'T3_I9', 'N3_I9', 'N9_I9', 'N15_I9', 'N21_I9',\n",
    "                 'T15_I15', 'T9_I15', 'T3_I15', 'N3_I15', 'N9_I15',\n",
    "                 'N15_I15', 'T9_I21', 'T3_I21', 'N3_I21', 'N9_I21'\n",
    "                ]\n",
    "        blind_spots = [19, 28]\n",
    "        \n",
    "    else:\n",
    "        ORDER = ['N9_S21', 'N3_S21', 'T3_S21', 'T9_S21', 'N15_S15',\n",
    "                 'N9_S15', 'N3_S15', 'T3_S15', 'T9_S15', 'T15_S15',\n",
    "                 'N21_S9', 'N15_S9', 'N9_S9', 'N3_S9', 'T3_S9',\n",
    "                 'T9_S9', 'T15_S9', 'T21_S9', 'N27_S3', 'N21_S3',\n",
    "                 'N15_S3', 'N9_S3', 'N3_S3', 'T3_S3', 'T9_S3',\n",
    "                 'T15_S3', 'T21_S3', 'N27_I3', 'N21_I3', 'N15_I3',\n",
    "                 'N9_I3', 'N3_I3', 'T3_I3', 'T9_I3', 'T15_I3',\n",
    "                 'T21_I3', 'N21_I9', 'N15_I9', 'N9_I9', 'N3_I9',\n",
    "                 'T3_I9', 'T9_I9', 'T15_I9', 'T21_I9', 'N15_I15',\n",
    "                 'N9_I15', 'N3_I15', 'T3_I15', 'T9_I15', 'T15_I15',\n",
    "                 'N9_I21', 'N3_I21', 'T3_I21', 'T9_I21']\n",
    "        blind_spots = [25, 34]\n",
    "            \n",
    "    # Define a helper function for Total Deviation and Pattern Deviation Points\n",
    "    def _subset_points(TDPD):\n",
    "        \n",
    "        TD = TDPD[:4] + TDPD[8:14] + TDPD[20:28] + TDPD[36:44] + TDPD[52:60]\\\n",
    "        + TDPD[68:76] + TDPD[84:90] + TDPD[96:100]\n",
    "        PD = TDPD[4:8] + TDPD[14:20] + TDPD[28:36] + TDPD[44:52] + TDPD[60:68]\\\n",
    "        + TDPD[76:84] + TDPD[90:96] + TDPD[100:]\n",
    "        \n",
    "        return TD, PD\n",
    "            \n",
    "    # Remove age and subset the points accordingly\n",
    "    POINTS = POINTS[1:]\n",
    "    \n",
    "    if len(POINTS) == 158:\n",
    "        THR  = POINTS[:54]\n",
    "        TDPD = POINTS[54:]\n",
    "        \n",
    "        TD, PD = _subset_points(TDPD)\n",
    "        \n",
    "        del THR[blind_spots[1]]\n",
    "        del THR[blind_spots[0]]\n",
    "    \n",
    "    elif (int(POINTS[53])) <= 3 and (len(POINTS) == 157):\n",
    "        THR  = POINTS[:53]\n",
    "        TDPD = POINTS[53:]\n",
    "        \n",
    "        TD, PD = _subset_points(TDPD)\n",
    "        \n",
    "        del THR[blind_spots[0]]\n",
    "    \n",
    "    else:\n",
    "        THR    = POINTS[:54]\n",
    "        \n",
    "        del THR[blind_spots[1]]\n",
    "        del THR[blind_spots[0]]\n",
    "        \n",
    "        TD, PD = close_up(s3BucketName, filename)\n",
    "        \n",
    "    # If the TD and PD lengths aren't correct, try a different mode\n",
    "    if (len(TD) != 52) or (len(PD) != 52):\n",
    "        TD, PD = close_up(s3BucketName, filename, 2)\n",
    "        \n",
    "    ORDER.remove(\"T15_S3\")\n",
    "    ORDER.remove(\"T15_I3\")\n",
    "\n",
    "    thr_order = [x + \"_Thr\" for x in ORDER]\n",
    "    td_order  = [x + \"_TD\" for x in ORDER]\n",
    "    pd_order  = [x + \"_PD\" for x in ORDER]\n",
    "\n",
    "    # Populate the data frame with thr points\n",
    "    output[thr_order] = THR\n",
    "\n",
    "    # Populate the data frame with td points\n",
    "    try:\n",
    "        output[td_order] = TD\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Populate the data frame with pd points\n",
    "    try:\n",
    "        output[pd_order] = PD\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Populate the data frame with other acquired info\n",
    "    output[\"Eye\"]              = Eye\n",
    "    output[\"TestType\"]         = TestType\n",
    "    output[\"TestPattern\"]      = TestPattern\n",
    "    output[\"TrialRXSphereRaw\"] = TrialRXSphereRaw\n",
    "    output[\"TrialRXCylRaw\"]    = TrialRXCylRaw\n",
    "    output[\"TrialRXAxisRaw\"]   = TrialRXAxisRaw\n",
    "\n",
    "    output[\"InstrumentModel\"]  = InstrumentModel\n",
    "    output[\"InstrumentSerialNumber\"] = InstrumentSerialNumber\n",
    "    output[\"InstrumentSoftwareVersion\"] = InstrumentSoftwareVersion\n",
    "    \n",
    "    try:\n",
    "        output[\"LowPatientReliabilityStatus\"] = LowPatientReliabilityStatus\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    return output, TestPattern, ORDER, blind_spots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa3ae20",
   "metadata": {},
   "source": [
    "#### Probability Points\n",
    "\n",
    "The function below uses average pixel values in the pdf to determine the values of the probability points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "22ae2b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_points(filename, start1, end1, start2, end2, box_width):\n",
    "    \n",
    "    # Convert the pdf to an image\n",
    "    img = np.array(convert_from_path(os.path.join(input_dir, filename))[0])\n",
    "    \n",
    "    # Loop through each box and calculate mean pixel value\n",
    "    PROBS = []\n",
    "    for i in np.arange(start1, end1, box_width):\n",
    "        for j in np.arange(start2, end2, box_width):\n",
    "            \n",
    "            point = img[i:(i+box_width), j:(j+box_width)]\n",
    "            value = point[2:(box_width - 2)].mean()\n",
    "            \n",
    "            # Determine the probability value according to the mean pixel value\n",
    "            if value == 255.0:\n",
    "                continue\n",
    "            if value < 155.0:\n",
    "                PROBS.append(\"P < 0.5%\")\n",
    "            if (value >= 155.0 and value < 190.0):\n",
    "                PROBS.append(\"P < 1%\")\n",
    "            if (value >= 190.0 and value < 225.0):\n",
    "                PROBS.append(\"P < 2%\")\n",
    "            if (value >= 225.0 and value < 250.0):\n",
    "                PROBS.append(\"P < 5%\")\n",
    "            if (value >= 250.0 and value < 255.0):\n",
    "                PROBS.append(\"Not Significant\")\n",
    "                \n",
    "    return PROBS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c207a4b",
   "metadata": {},
   "source": [
    "#### Extracting information from the pdfs\n",
    "\n",
    "The code below puts everything together; text will be extracted from the pdfs, cleaned, then formatted as a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "59aa3421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 PDF(s) processed\n",
      "2 PDF(s) processed\n",
      "3 PDF(s) processed\n"
     ]
    }
   ],
   "source": [
    "# Start up the client\n",
    "client = boto3.client('textract', 'us-east-1')\n",
    "\n",
    "CSVS_242 = []\n",
    "for i, x in enumerate(files):\n",
    "    \n",
    "    # Extract text\n",
    "    text = extract_text(s3BucketName, x)\n",
    "    \n",
    "    # Generate the first round of outputs\n",
    "    output = key_value_output(text)\n",
    "    \n",
    "    # Generate the second round of outputs\n",
    "    output, TestPattern, ORDER, blind_spots = raw_text_output(output, text, x)\n",
    "    \n",
    "    # Calculate the probability points for both plots\n",
    "    TDP = probability_points(x, 1498, 1762, 188, 485, 33)\n",
    "    PDP = probability_points(x, 1498, 1762, 692, 989, 33)\n",
    "\n",
    "    tdp_order = [x + \"_TDP\" for x in ORDER]\n",
    "    pdp_order = [x + \"_PDP\" for x in ORDER]\n",
    "    \n",
    "    output[tdp_order] = TDP\n",
    "    output[pdp_order] = PDP\n",
    "    \n",
    "    # Append to a list based on test type\n",
    "    if TestPattern == \"Central 24-2\":\n",
    "        CSVS_242.append(output)\n",
    "        \n",
    "    print(str(i + 1) + \" PDF(s) processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "eba26049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the batch of files as a csv and delete the images folder\n",
    "DF = pd.concat(CSVS_242)\n",
    "DF.to_csv(os.path.join(output_dir, \"24_2.csv\"), index = False)\n",
    "\n",
    "shutil.rmtree(os.path.join(input_dir, \"images\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
